# Game Optimizer V5.0 - ML Configuration
# Configuration for ML Pipeline, AutoML, and Model Deployment

# Model Directories
model_dir: "~/.game_optimizer/ml_models_v5"
cache_dir: "~/.game_optimizer/ml_cache"
logs_dir: "~/.game_optimizer/ml_logs"

# Feature Engineering
feature_engineering:
  enabled: true
  num_features: 96
  scaler_type: "robust"  # "robust", "standard", "minmax"
  
  # Feature groups
  hardware_features: 15
  config_features: 20
  performance_features: 30
  network_disk_features: 6
  derived_features: 25

# Deep Learning (PyTorch)
deep_learning:
  enabled: true
  model_type: "resnet"
  
  # Architecture
  input_size: 96
  encoder_layers: [256, 256, 128, 128, 64]
  dropout_rates: [0.3, 0.3, 0.2, 0.2, 0.2]
  use_batch_norm: true
  use_residual_blocks: true
  
  # Training
  epochs: 100
  batch_size: 64
  learning_rate: 0.001
  weight_decay: 0.0001
  optimizer: "adamw"  # "adam", "adamw", "sgd"
  
  # Scheduling
  scheduler: "cosine"  # "cosine", "step", "plateau"
  warmup_epochs: 5
  
  # Multi-task learning weights
  loss_weights:
    fps: 0.5
    stability: 0.3
    success: 0.2
  
  # Early stopping
  early_stopping:
    enabled: true
    patience: 10
    min_delta: 0.001

# Gradient Boosting Ensemble
gradient_boosting:
  enabled: true
  
  # XGBoost
  xgboost:
    enabled: true
    n_estimators: 500
    max_depth: 8
    learning_rate: 0.05
    subsample: 0.8
    colsample_bytree: 0.8
    
  # LightGBM
  lightgbm:
    enabled: true
    num_leaves: 127
    learning_rate: 0.05
    n_estimators: 500
    
  # CatBoost
  catboost:
    enabled: true
    iterations: 500
    depth: 8
    learning_rate: 0.05
  
  # Ensemble voting
  voting:
    method: "weighted"  # "weighted", "uniform"
    auto_weights: true  # Calculate weights from validation performance

# AutoML (Optuna)
automl:
  enabled: false  # Disable by default (expensive)
  n_trials: 100
  timeout: 3600  # 1 hour
  pruning: true
  
  # Search space
  search_space:
    learning_rate: [0.00001, 0.1]  # log scale
    batch_size: [16, 32, 64, 128, 256]
    hidden_layers: [2, 3, 4, 5]
    hidden_units: [64, 128, 256, 512]
    dropout: [0.0, 0.5]
    weight_decay: [0.000001, 0.001]

# Explainability
explainability:
  enabled: true
  
  # SHAP
  shap:
    enabled: true
    max_samples: 100  # For SHAP calculation
    top_k_features: 5
    
  # LIME
  lime:
    enabled: false
    num_samples: 1000

# Model Monitoring
monitoring:
  enabled: true
  
  # Drift detection
  drift_detection:
    enabled: true
    method: "psi"  # "psi", "ks", "adwin"
    threshold: 0.2
    check_interval: 100  # Check every N predictions
  
  # Performance tracking
  track_metrics: true
  alert_on_degradation: true
  degradation_threshold: 0.15  # 15% degradation triggers alert

# Auto-retraining
auto_retraining:
  enabled: true
  
  # Triggers
  triggers:
    min_sessions: 500  # Minimum new sessions before retraining
    time_interval: 604800  # 1 week in seconds
    drift_detected: true
    performance_degradation: true
  
  # Strategy
  strategy: "incremental"  # "incremental", "full"
  validation_split: 0.2
  rollback_on_regression: true
  regression_threshold: 0.05  # 5% worse = rollback

# Production Deployment
production:
  # ONNX
  onnx:
    enabled: false  # Not implemented yet
    quantization: "int8"  # "fp32", "fp16", "int8"
    optimize_graph: true
    
  # API
  api:
    enabled: false  # Not implemented yet
    host: "127.0.0.1"
    port: 8000
    workers: 4
    
  # Caching
  cache:
    enabled: false  # Not implemented yet
    backend: "redis"
    ttl: 3600  # 1 hour

# Data Collection
data_collection:
  enabled: true
  min_session_duration: 60  # seconds
  max_sessions_stored: 10000
  
  # Privacy
  anonymize_data: true
  exclude_personal_info: true

# Validation
validation:
  test_size: 0.2
  cross_validation:
    enabled: false
    n_folds: 5
    
  # Metrics thresholds (success criteria)
  success_criteria:
    fps_mape: 0.08  # < 8% error
    stability_r2: 0.85  # > 0.85 RÂ²
    success_f1: 0.90  # > 0.90 F1-score
    
# Performance
performance:
  inference_latency_ms: 10  # Target < 10ms
  training_time_hours: 2  # Target < 2 hours for 10K samples
  model_size_mb: 100  # Target < 100MB

# Logging
logging:
  level: "INFO"  # "DEBUG", "INFO", "WARNING", "ERROR"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  file: "ml_pipeline.log"
  rotate: true
  max_bytes: 10485760  # 10MB
  backup_count: 5

# Random Seeds (for reproducibility)
random_seeds:
  global: 42
  train_split: 42
  model_init: 42
